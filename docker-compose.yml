services:
  zookeeper:
    profiles: [test, production]
    image: bitnami/zookeeper:latest
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    networks:
      - umbrix_network

  kafka:
    profiles: [test, production]
    image: bitnami/kafka:latest # Simplified for local testing, consistent with test.yml
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_started
    ports:
      - "9092:9092"
    environment:
      # Auto-create key topics with 1 partition, 1 replica for simplicity in local dev
      - KAFKA_CREATE_TOPICS=raw.intel:1:1,enriched.intel:1:1,graph.events:1:1,feeds.discovered:1:1,agent.errors:1:1,coordination.tasks:1:1,raw.intel.taxii:1:1,raw.intel.misp:1:1,normalized.intel:1:1
      # KRaft mode settings for Bitnami Kafka (single node)
      - KAFKA_CFG_NODE_ID=1 # Required for KRaft mode
      - KAFKA_CFG_PROCESS_ROLES=broker,controller # Node acts as both broker and controller
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092 # Use service name for clients within docker network
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093 # Points to itself
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true # Good for development
      # Zookeeper settings (Bitnami image might still try to use ZK if these are present,
      # but KRaft settings should take precedence if image version supports it well)
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 # Keep for now, but KRaft aims to remove ZK dependency
    healthcheck:
      test: ["CMD", "bash", "-c", "echo > /dev/tcp/localhost/9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - umbrix_network

  kafka-connect:
    profiles: [test, production]
    image: confluentinc/cp-kafka-connect:7.2.1
    container_name: kafka_connect
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      - CONNECT_REST_ADVERTISED_HOST_NAME=kafka-connect # Use service name
      - CONNECT_BOOTSTRAP_SERVERS=kafka:9092
      - CONNECT_REST_PORT=8083
      - CONNECT_GROUP_ID=connect-cluster
      - CONNECT_CONFIG_STORAGE_TOPIC=connect-configs
      - CONNECT_OFFSET_STORAGE_TOPIC=connect-offsets
      - CONNECT_STATUS_STORAGE_TOPIC=connect-status
      - CONNECT_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      # Set replication factor for internal topics to 1 for single-broker setup
      - CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR=1
      - CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR=1
      - CONNECT_STATUS_STORAGE_REPLICATION_FACTOR=1
      # Set number of partitions for internal topics (defaults are often 1 or 25)
      - CONNECT_CONFIG_STORAGE_PARTITIONS=1
      - CONNECT_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_INTERNAL_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_INTERNAL_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      # - CONNECT_PLUGIN_PATH=/usr/share/java # Keep if you have custom connectors for local testing
      # Removed GCS specific parts for a general local test setup
    networks:
      - umbrix_network

  neo4j:
    profiles: [test, production]
    image: neo4j:4.4 # Consistent with kind and test.yml
    container_name: neo4j
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=${NEO4J_AUTH}
      # Enable APOC procedures
      - NEO4JLABS_PLUGINS=["apoc"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*
    healthcheck:
      test: ["CMD", "true"]
      interval: 10s
      timeout: 2s
      retries: 3
    networks:
      - umbrix_network

  postgres:
    profiles: [test, production]
    image: postgres:15
    env_file:
      - .env
    container_name: postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    volumes:
      - postgres-data:/var/lib/postgresql/data

    networks:
      - umbrix_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5

  sqlx_prepare:
    profiles: [test]
    build:
      context: cti_backend
      dockerfile: Dockerfile.sqlx_prepare
    working_dir: /app
    volumes:
      - ./cti_backend:/app
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - DATABASE_URL=postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - CARGO=cargo
    networks:
      - umbrix_network
    entrypoint:
      - "/bin/sh"
      - "-c"
      - |
        # wait for Postgres to be ready
        until pg_isready -h postgres -U "${POSTGRES_USER}"; do
          echo "Waiting for Postgres..."
          sleep 1
        done
        # apply all migrations so tables exist
        cargo sqlx migrate run
        # generate offline SQLx metadata
        cargo sqlx prepare -- --lib

  cti_backend:
    profiles: [test, production]
    build:
      context: ./cti_backend
      dockerfile: Dockerfile
    container_name: cti_backend
    ports:
      - "8088:8080"
    env_file:
      - .env
    environment:
      - NEO4J_URI=${NEO4J_URI}
      - NEO4J_USER=${NEO4J_USER}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - GEMINI_MODEL_NAME=${GEMINI_MODEL_NAME:-gemini-1.5-flash-latest}
      - CTI_API_KEY=${CTI_API_KEY}
      - AGENT_BOOTSTRAP_KEY=${CTI_API_KEY}
      - DATABASE_URL=postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - JWT_SECRET=${JWT_SECRET}
      - STRIPE_SECRET_KEY=${STRIPE_SECRET_KEY}
      - STRIPE_PUBLISHABLE_KEY=${STRIPE_PUBLISHABLE_KEY}
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - RUST_LOG=info,cti_backend=debug
      - APP_PORT=8080
      # Optional per-IP rate limiting (Layer-0). Comment-out or set to "" to disable.
      - IP_RATE_LIMIT_RPS=${IP_RATE_LIMIT_RPS:-100}
      - IP_RATE_LIMIT_BURST=${IP_RATE_LIMIT_BURST:-200}
    # Dev-only code mount removed for production
    depends_on:
      neo4j:
        condition: service_healthy
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
    # Disable healthcheck for the offline/test-only stub
    healthcheck:
      test: ["CMD", "bash", "-c", "curl -f http://localhost:8080/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - umbrix_network

  web:
    profiles: [test, production]
    build:
      context: ./web
      dockerfile: Dockerfile # Assuming Dockerfile is in ./web
    ports:
      - "8082:80" # Expose web UI on host port 8082
    env_file:
      - .env
    environment:
      # For local Docker Compose, point Nginx to the backend container
      - CTI_BACKEND_UPSTREAM_FOR_NGINX_API=http://cti_backend:8080/api/
      - CTI_BACKEND_URL=/api
    depends_on:
      cti_backend:
        condition: service_healthy
      prometheus:
        condition: service_started
      grafana:
        condition: service_started
      loki:
        condition: service_started
    volumes:
      - ./htpasswd/htpasswd:/etc/nginx/.htpasswd:ro # Mount the generated htpasswd file
      # Dev-only docs mount removed for production
    networks:
      - umbrix_network

  agents:
    profiles: [production]
    build:
      context: ./agents
      dockerfile: Dockerfile
    container_name: agents
    env_file:
      - .env
    depends_on:
      cti_backend:
        condition: service_healthy
      kafka:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      - MISTRAL_API_KEY=${MISTRAL_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - CTI_BACKEND_URL=http://cti_backend:8080/internal
      - CTI_API_KEY=${CTI_API_KEY}
      - PYTHONPATH=/app
      - AGENT_CONFIG_PATH=/app/config.yaml
      - UV_LINK_MODE=copy
    # Mount full repository so Python sees the `agents` package directory
    # working_dir: /app
    # volumes:
    #   - ./agents:/app
    networks:
      - umbrix_network

  intelligent-crawler-agent:
    profiles: [production]
    build:
      context: ./agents
      dockerfile: discovery_agent/Dockerfile.intelligent_crawler
    container_name: intelligent-crawler-agent
    env_file:
      - .env
    depends_on:
      cti_backend:
        condition: service_healthy
      kafka:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      redis:
        condition: service_started
    environment:
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - CTI_BACKEND_URL=http://cti_backend:8080/api
      - CTI_API_KEY=${CTI_API_KEY}
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - REDIS_URL=redis://redis:6379
      - PYTHONPATH=/app
      - AGENT_CONFIG_PATH=/app/config.yaml
      - RUST_LOG=info,intelligent_crawler=debug
    # Dev-only code mount removed for production
    ports:
      - "8081:8080"  # Expose metrics port
    networks:
      - umbrix_network

  prometheus: # New service
    profiles: [test, production]
    restart: unless-stopped
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"  # Expose prometheus internally for nginx proxy
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml # Mount our config
      - ./config/prometheus-rules.yml:/etc/prometheus/prometheus-rules.yml # Mount rules
      - prometheus-data:/prometheus # Persistent data
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--web.external-url=/admin/prometheus/"
      - "--web.route-prefix=/"
    depends_on:
      cti_backend:
        condition: service_healthy
    networks:
      - umbrix_network

  grafana: # New service
    profiles: [test, production]
    restart: unless-stopped
    image: grafana/grafana:latest
    container_name: grafana
    # No direct port exposure - only accessible through Nginx proxy
    volumes:
      - grafana-data:/var/lib/grafana # Persistent data
      - ./grafana/provisioning:/etc/grafana/provisioning # Provisioning config
      - ./grafana/dashboards:/var/lib/grafana/dashboards # Dashboards
    environment:
      - GF_SECURITY_ADMIN_USER=${GF_SECURITY_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD:-StrongGrafanaPassword123!}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=/admin/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_AUTH_PROXY_ENABLED=true
      - GF_AUTH_PROXY_HEADER_NAME=X-WEBAUTH-USER
      - GF_AUTH_PROXY_HEADER_PROPERTY=username
      - GF_AUTH_PROXY_AUTO_SIGN_UP=true
      - GF_DASHBOARDS_MIN_REFRESH_INTERVAL=5s
    depends_on:
      prometheus:
        condition: service_started
      loki:
        condition: service_started
    networks:
      - umbrix_network

  loki:
    profiles: [test, production]
    image: grafana/loki:latest
    container_name: loki
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - loki-data:/loki
    networks:
      - umbrix_network

  promtail:
    profiles: [test, production]
    restart: on-failure
    image: grafana/promtail:latest
    container_name: promtail
    volumes:
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./config/promtail-config.yml:/etc/promtail/config.yml
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      - loki
    networks:
      - umbrix_network

  redis:
    profiles: [test, production]
    image: redis:7
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - umbrix_network


  # Test suite runner for agents
  test_agents:
    image: python:3.12-slim
    profiles: [test]
    working_dir: /app
    volumes:
      - .:/app
      - ./reports/agents:/app/reports
    entrypoint: ["/bin/bash", "-lc"]
    command: >
      cd agents &&
      uv sync &&
      mkdir -p /app/reports/agents &&
      uv run pytest tests --junitxml=/app/reports/agents/agents-results.xml -v
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=neo4jpassword
      - PYTHONPATH=/app/agents
      - AGENT_CONFIG_PATH=/app/agents/config.yaml
    depends_on:
      kafka:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      postgres:
        condition: service_healthy

  # Test suite runner for cti_backend
  test_backend:
    image: rust:slim-bullseye
    profiles: [test]
    working_dir: /workspace/cti_backend
    volumes:
      - ./cti_backend:/workspace/cti_backend
      - ~/.cargo:/usr/local/cargo
      - ./reports/backend:/workspace/cti_backend/reports
      - ./reports/backend-coverage:/workspace/cti_backend/coverage-report
    entrypoint: ["/bin/bash", "-lc"]
    # We explicitly wait for Postgres to report healthy before running any
    # database-related commands.  This eliminates a race where the container
    # would previously attempt to create the test user/database before the
    # server was accepting connections, leading to intermittent failures in
    # CI and local runs.
    command: >
      echo "Waiting for Postgres to become ready..." &&
      until PGPASSWORD=${POSTGRES_PASSWORD} pg_isready -h postgres -U ${POSTGRES_USER}; do
        sleep 2;
      done &&
      echo "Postgres is ready — running tests" &&
      cd /workspace/cti_backend &&
      mkdir -p /workspace/cti_backend/reports /workspace/cti_backend/coverage-report &&
      cargo install cargo-tarpaulin &&
      # Create test user and database if needed (idempotent)
      PGPASSWORD=${POSTGRES_PASSWORD} psql -h postgres -U ${POSTGRES_USER} -tc "SELECT 1 FROM pg_roles WHERE rolname='test_user'" | grep -q 1 || \
        PGPASSWORD=${POSTGRES_PASSWORD} psql -h postgres -U ${POSTGRES_USER} -c "CREATE USER test_user WITH PASSWORD 'test_password' CREATEDB;" &&
      PGPASSWORD=${POSTGRES_PASSWORD} psql -h postgres -U ${POSTGRES_USER} -tc "SELECT 1 FROM pg_database WHERE datname='test_cti'" | grep -q 1 || \
        PGPASSWORD=${POSTGRES_PASSWORD} psql -h postgres -U ${POSTGRES_USER} -c "CREATE DATABASE test_cti OWNER test_user;" &&
      # Generate SQLx metadata & run migrations so that the schema is ready for integration tests
      # Ensure sqlx-cli is available for running migrations in the next step
      cargo install sqlx-cli --no-default-features --features postgres --version ^0.7 &&
      cargo sqlx migrate run &&
      # Run tarpaulin for all tests including integration tests. Increasing the
      # timeout from 300 s → 600 s gives the coverage pass a bit more headroom
      # on slower CI executors without impacting healthy local runs.
      cargo tarpaulin --skip-clean --out Html --output-dir coverage-report --timeout 600 --skip-clean &&
      cargo test --features postgres_tests -- --test-threads=1 --format json > /workspace/cti_backend/reports/backend-results.json
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      # Use values coming from `.env` (Compose automatically loads it).  This
      # avoids duplicating credentials in the compose file.
      - DATABASE_URL=${DATABASE_URL}
      - TEST_DATABASE_URL=${DATABASE_URL}
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=neo4jpassword
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - JWT_SECRET=test-jwt-secret-for-running-tests
      - RUST_LOG=info,cti_backend=debug
      - CTI_API_KEY=test-api-key-for-integration-tests
      - APP_HOST=0.0.0.0
      - APP_PORT=8080
    depends_on:
      neo4j:
        condition: service_healthy
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
      cti_backend:
        condition: service_healthy

  # Test suite runner for the web frontend (Vitest integration & unit tests)
  test_web:
    image: node:18-slim
    profiles: [test]
    working_dir: /workspace/web
    volumes:
      - ./web:/workspace/web
      - ./reports/web:/workspace/web/tests/reports
    entrypoint: ["/bin/bash", "-lc"]
    command: >
      npm ci && \
      # Run unit + integration tests separately so we can produce distinct reports if desired
      npm run test:run && \
      npm run test:integration -- --reporter=json --outputFile=tests/reports/web-integration-results.json
    environment:
      # Point the JS client directly at the backend service within the compose network
      - CTI_BACKEND_URL=http://cti_backend:8080/internal
      - NODE_ENV=test
    depends_on:
      cti_backend:
        condition: service_healthy

volumes:
  # grafana-storage: # Removed as Grafana is removed for this test setup
  prometheus-data: {} # New volume for Prometheus
  grafana-data: {} # New volume for Grafana
  loki-data: {} # New volume for Loki
  postgres-data: # Renamed from postgres-test-data for consistency with service name
  redis-data: {} # Added for Redis storage

networks: # New network definition
  umbrix_network:
    driver: bridge
